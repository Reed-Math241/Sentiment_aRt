[
  {
    "path": "posts/2021-03-07-mini-project-2-instructions/",
    "title": "Sentiment aRt",
    "description": "Using sentiment analysis to make reproducible data art.",
    "author": [
      {
        "name": "Amrita K. Sawhney",
        "url": {}
      },
      {
        "name": "Maxwell J.D. VanLandschoot",
        "url": {}
      }
    ],
    "date": "2021-05-11",
    "categories": [
      "Art",
      "Books"
    ],
    "contents": "\nWhat is art? Now, not to get too pithy or metaphysical, art is really just finding a way to tell a story or convey some message to an audience. To us, and probably many other data scientists out there, data innately carries with it this artistic quality. While a .csv full of numbers and variables might not have the same accessible and aesthetic qualities we are used to in the art we consume, there is nonetheless beauty to be found. With this project we wanted to go beyond our usual representations of data –graphs and regressions– and present another way with which to view it.\nThe data we chose to serve as the basis of our art are sentiment analyses from various books in the Gutenberg Library (an example code for a sentiment analysis can be found below). We did this for the primary reason that the sentiment of a book is difficult to plot on a graph or sum nicely into a statistic. It’s not impossible, of course, because I can tell you, for example, that the overall sentiment of The War of the Worlds is -0.393 and the most impactful word is “like.” But we don’t feel like you get the full picture of what the book feels like just from those statistics. Our various attempts at art below will hopefully serve to help bridge that gap between data and understanding. Or, at the very least, we hope to give you a glimpse into the world of data art and the capabilities of R.\n\n\nlibrary(tidyverse)\nlibrary(gutenbergr)\n\ntext <- gutenberg_download(36) %>%\n  mutate(text = str_to_lower(text, locale = \"en\"))\n\nafinn <- get_sentiments(\"afinn\")\n\ntext_afinn <- text %>%\n  unnest_tokens(output = word, input = text, token = \"words\") %>%\n  count(word, sort = TRUE) %>%\n  inner_join(afinn, by = \"word\") %>% \n  mutate(prob = n/sum(n),\n         impact = abs(n * value)) %>%\n  arrange(desc(impact)) \n\nsentiment <- sum(text_afinn$n*text_afinn$value)/sum(text_afinn$n)\n\nsentiment\n\n\n\nBefore we can make any art, we need to first collect our relevant data. As we mentioned previously, we are using texts from the Gutenberg Library with the gutenbergr library. For these first two pieces, we’re using the book “Stories for Ninon” by Émile Zola. There, honestly, is not a deep reason behind why we’re starting with this book, the results were just striking. With a text chosen, we download it using ‘gutenberg_download’. Once the book is downloaded we “tokenize” it by breaking it into individual words. From this we obtain the affinity score for each word. An affinity score is a -5 to 5 ranking given to each word that shows just how negative or positive any given word is. For example, “breathtaking” gets a rating of 5, while a word like “catastrophic” gets a -4 (I would give an example of a word with a -5 rating, trust me there are a few, but they are all too profane to put into a blog).\n\nShow code\nlibrary(tidyverse)\nlibrary(gutenbergr)\nlibrary(tidytext)\n\ntext_num <- 7462\n\ntext <- gutenberg_download(text_num) %>%\n  mutate(text = str_to_lower(text, locale = \"en\"))\n\nafinn <- get_sentiments(\"afinn\")\n\ntext_afinn <- text %>%\n  unnest_tokens(output = word, input = text, token = \"words\") %>%\n  count(word, sort = TRUE) %>%\n  inner_join(afinn, by = \"word\") %>% \n  mutate(prob = n/sum(n)) %>%\n  arrange(desc(n)) \n\ntext_neg <- text_afinn %>%\n  filter(value < 0) %>%\n  arrange(desc(n))\n\ntext_pos <- text_afinn %>%\n  filter(value > 0) %>%\n  arrange(desc(n))\n\n\n\nAfter we calculate individual sentiments, we sum negative, positive, and all values divided by the number of words counted to obtain three different sentiment scores which will be applicable later. These three scores tell us three things, how negative the negative elements of the text are, how positive the positive elements of the text are, and how positive or negative it is overall. The last step here multiplies the sentiment score by 10000, rounds it to the nearest whole number, and takes the absolute value as to only output positive values. This may seem odd, but this value will be used to set the “seed” for future plots where randomization is used so that our art is reproducible, and R only uses seeds between 1 and 10000.\n\nShow code\nsenti_neg <- sum(text_neg$n*text_neg$value)/sum(text_neg$n)\n\nsenti_pos <- sum(text_pos$n*text_pos$value)/sum(text_pos$n)\n\nsenti_raw <- sum(text_afinn$n*text_afinn$value)/sum(text_afinn$n) \n\nsenti <- abs(round(senti_raw * 10000))\n\n\n\nThe last thing, before attempting to create anything, is to build a way to include colors in our art. We wanted our texts to dictate what colors are used, and since it would be infeasible and impractical to manually sort for colors, we needed to create a way to identify every color within our texts. The process is fairly simple using the ‘stringr’ function ‘str_extract’, all we need to do is provide a list of colors to compare to the text. There are two ways we could do this, the first is to copy and paste over a list of the hundred or so most common colors. The second way, which we elected to do, is to scrape Wikipedia’s lists of all colors –over 800 total. Our next step is to go back to our “tokenization” of the text which breaks our text into individual words. If we were to only tokenize our text into individual words while looking for color we would miss all of the lovely two and three word colors like ‘international orange’ or ‘deep space sparkle’. Now, we don’t think we will run across many, if any, of these odd colors, but we wanted to be thorough. To ensure we did not lose any colors we broke our text up into n-grams of length 3, a fancy way of saying into three-word chunks. We are, then, left with every three-word combination that appears in the text. This process takes longer and is more computationally demanding than with typical one-word tokenizing, but, again, we wanted to be extra sure we are getting every color. With this list of colors we can compare it to our text and pull out every matching instance of a color. Corresponding hexadecimal codes are also added to account for non-standard colors.\n\nShow code\nlibrary(rvest)\nlibrary(httr)\nlibrary(stringr)\n\nurl <- \"https://en.wikipedia.org/wiki/List_of_colors:_A%E2%80%93F\"\ntables <- url %>%\n  read_html() %>%\n  html_nodes(css = \"table\")\ncolors1 <- html_table(tables[[1]], fill = TRUE)\n\nurl <- \"https://en.wikipedia.org/wiki/List_of_colors:_G%E2%80%93M\"\ntables <- url %>%\n  read_html() %>%\n  html_nodes(css = \"table\")\ncolors2 <- html_table(tables[[1]], fill = TRUE)\n\nurl <- \"https://en.wikipedia.org/wiki/List_of_colors:_N%E2%80%93Z\"\ntables <- url %>%\n  read_html() %>%\n  html_nodes(css = \"table\")\ncolors3 <- html_table(tables[[1]], fill = TRUE)\n\ncolors_scrape <- rbind(colors1, colors2, colors3) %>%\n  mutate(Name = str_to_lower(Name, locale = \"en\"),\n         Name = str_replace(Name, \" \\\\s*\\\\([^\\\\)]+\\\\)\", \"\")) %>%\n  distinct(Name, .keep_all = TRUE)\n\ncolors <- colors_scrape %>%\n  select(Name)\n\ncolors_all <- colors %>%\n  summarise(Name = paste0(Name, collapse = \"|\")) %>%\n  as.character(expression())\n\ncolors_all <- paste0(\"\\\\b(\", colors_all, \")\\\\b\")\n\ntext_grammed <- text %>%\n  unnest_tokens(output = word, input = text,\n  token = \"ngrams\", n = 3)\n\ncolors_parsed <- text_grammed %>%\n  mutate(Name = str_extract(word, pattern = colors_all)) %>%\n  drop_na() %>%\n  select(Name)\n\ncolors_counted <- colors_parsed %>%\n  group_by(Name) %>%\n  count() %>%\n  ungroup() %>%\n  mutate(prob = n/sum(n)) %>%\n  arrange(desc(n))\n  \ncolors_final <- left_join(colors_counted, colors_scrape, by = \"Name\") %>%\n  select(Name, `Hex(RGB)`, n, prob) %>%\n  rename(Hex = `Hex(RGB)`) \n\n\n\nThe first art generation tool we used was inspired by the ‘generativeart’ library. This works by using random number generation –based on the sentiment score of the book– and the positive and negative sentiment values calculated previously. So, each artwork produced with this method is unique and dependent on that specific text’s sentiment. This also means that texts with more extreme sentiments tend to yield more varied results. For this piece we also wanted its colors and title to be rooted in the text. For the colors, they are simply the two most common colors to appear in the text. For the title, we randomly sampled three words from the sentiment analysis, based on their proportion of appearance in the sample, so words that appear more often in the text are more likely to appear in the title. The lovely generated title for this next piece is “Grand Commit Fan.” For aesthetic reasons we chose to not include the title on our output, though this is toggleable through the function with ‘title = TRUE’ and it will put the generated name in the bottom right corner.\n\nShow code\nmy_formula <- list(\n  x = quote(runif(1, -1, 1) * x_i^2 - senti_neg * sin(y_i^2)),\n  y = quote(runif(1, -1, 1) * y_i^2 - senti_pos * cos(x_i^2))\n)\n\ncolor_main <- colors_final %>%\n  filter(row_number() == 1) %>%\n  select(Hex) %>%\n  as.character()\n  \ncolor_background <- colors_final %>%\n  filter(row_number() == 2) %>%\n  select(Hex) %>%\n  as.character()\n\ngenerative_art <- function(formula = my_formula, \n                           polar = TRUE, \n                           title = TRUE, \n                           main_color = color_main, \n                           background_color = color_background) {\n  \n  require(tidyverse)\n  require(cowplot)\n  \n  set.seed(senti)\n  \n  df <- seq(from = -pi, to = pi, by = 0.01) %>%\n    expand.grid(x_i = ., y_i = .) %>%\n    mutate(!!!formula)\n\n    if (polar == TRUE) {\n    plot <- df %>%\n      ggplot(aes(x = x, y = y)) +\n      geom_point(alpha = 0.1, size = 0, shape = 20, color = main_color) +\n      theme_void() +\n      coord_fixed() +\n      coord_polar() +\n      theme(\n        panel.background = element_rect(fill = background_color, color = background_color),\n        plot.background = element_rect(fill = background_color, color = background_color)\n        )\n  } else {\n    plot <- df %>%\n      ggplot(aes(x = x, y = y)) +\n      geom_point(alpha = 0.1, size = 0, shape = 20, color = main_color) +\n      theme_void() +\n      coord_fixed() +\n      theme(\n        panel.background = element_rect(fill = background_color, color = background_color),\n        plot.background = element_rect(fill = background_color, color = background_color)\n        )\n  }\n  \n  if(title == TRUE) {\n    \n    title <- sample_n(text_afinn, 3, replace = TRUE, weight = prob) %>%\n      select(word) %>%\n      mutate(word = str_to_title(word))\n    \n    title <- title %>%\n        summarise(word = paste0(word, collapse = \" \")) %>%\n        as.character(expression())\n      \n    p <- ggdraw(plot) +\n      draw_label(title, x = 0.65, y = 0.05, hjust = 0, vjust = 0,\n                 fontfamily = \"Roboto Condensed\", fontface = \"plain\", color = color_main, size = 10)\n    \n    ggsave(\"genart.png\", plot = p)\n    \n    p\n    \n  } else{\n    ggsave(\"genart.png\", plot = plot)\n    \n    plot\n  }\n\n}\n\ngenerative_art(polar = TRUE, title = FALSE)\n\n\n\n\nThe code we used, an observant reader might note, can create art based on polar or cartesian coordinates. The example below uses cartesian coordinates and shows, just like with any form of data visualization, how author inputs can have a massive effect on the final product. All of the code for a non-polar plot is the exact same save for the exclusion of ‘coord_polar()’.\n\nShow code\ngenerative_art(polar = FALSE, title = FALSE)\n\n\n\n\nOur next attempt at data art is based off of the Jean Fan’s tunnel art. This plot does not use randomization like the ones above, rather it takes a series of rectangles and incrementally “pushes” back and rotates to create a spiral shape. The angle of the rotation is based upon the raw sentiment score of the text and the colors used are pulled from the text. We’re working with a different text than the previous pieces, so we’ll need to re-run most of our previous code. This time we’re working with “The Scarlett Letter” to hopefully get some striking colors. The only difference between these two swirling art pieces is how often the colors repeat. The first has no repetition, while the second has repetition based on the positive sentiment of the piece.\n\nShow code\ntext_num <- 33\n\ntext <- gutenberg_download(text_num) %>%\n  mutate(text = str_to_lower(text, locale = \"en\"))\n\nafinn <- get_sentiments(\"afinn\")\n\ntext_afinn <- text %>%\n  unnest_tokens(output = word, input = text, token = \"words\") %>%\n  count(word, sort = TRUE) %>%\n  inner_join(afinn, by = \"word\") %>% \n  mutate(prob = n/sum(n)) %>%\n  arrange(desc(n)) \n\ntext_neg <- text_afinn %>%\n  filter(value < 0) %>%\n  arrange(desc(n))\n\ntext_pos <- text_afinn %>%\n  filter(value > 0) %>%\n  arrange(desc(n))\n\nsenti_neg <- sum(text_neg$n*text_neg$value)/sum(text_neg$n)\n\nsenti_pos <- sum(text_pos$n*text_pos$value)/sum(text_pos$n)\n\nsenti_raw <- sum(text_afinn$n*text_afinn$value)/sum(text_afinn$n) \n\ntext_grammed <- text %>%\n  unnest_tokens(output = word, input = text,\n  token = \"ngrams\", n = 3)\n\ncolors_parsed <- text_grammed %>%\n  mutate(Name = str_extract(word, pattern = colors_all)) %>%\n  drop_na() %>%\n  select(Name)\n\ncolors_counted <- colors_parsed %>%\n  group_by(Name) %>%\n  count() %>%\n  ungroup() %>%\n  mutate(prob = n/sum(n)) %>%\n  arrange(desc(n))\n  \ncolors_final <- left_join(colors_counted, colors_scrape, by = \"Name\") %>%\n  select(Name, `Hex(RGB)`, n, prob) %>%\n  rename(Hex = `Hex(RGB)`) \n\n\n\n\nShow code\nlibrary(grDevices)\nlibrary(grid) \n\n# credit: https://jean.fan/art-with-code/portfolio/20180721_tunnel/\n\ncolors_swirl <- colors_final %>%\n  slice_head(n = 3)\n\n# Gradient Colors\nvp <- viewport(x = 0, y = 0, width = 1, height = 1)\ngrid.show.viewport(vp)\n\ncolors <- colorRampPalette(c(colors_swirl$Hex))(500)\n\npushViewport(viewport(width = 1, height = 1, angle = 0))\n\ngrid.rect(gp = gpar(col = NA, fill = colors[1]))\n\nfor(i in 2:500){\n  pushViewport(viewport(width = 0.99, height = 0.99, angle = 10*senti_raw))\n  grid.rect(gp = gpar(col = NA, fill = colors[i])\n  )\n}\n\n\n\n\n\nShow code\nvp <- viewport(x = 0, y = 0, width = 1, height = 1)\ngrid.show.viewport(vp)\n\ncolors_swirl <- colors_final %>%\n  slice_head(n = 3)\n\ncolors <- rep(colorRampPalette(c(colors_swirl$Hex))(100), round(senti_pos) + 1)\n\npushViewport(viewport(width = 1, height = 1, angle = 0))\n\ngrid.rect(gp = gpar(col = NA, fill = colors[1]))\n\nfor(i in 2:500){\n  pushViewport(viewport(width = 0.99, height = 0.99, angle = 10*senti_raw))\n  grid.rect(gp = gpar(col = NA, fill = colors[i])\n  )\n}\n\n\n\n\nOur next attempt at art was a little more straightforward. In the following 16 plots, we plotted every word with a sentiment from the beginning of the text to the end in order. Starting at the origin (marked with a red dot), a line of length 1 is drawn in the direction corresponding to the sentiment of the word. A line to the left means a negative sentiment, a line to the right means a positive sentiment. The more the line angles down, the stronger the sentiment. So, for example, a word with the sentiment of +1 would draw a line up and to the right. A word with the sentiment of -5 would draw a line down and to the left. This not only produces visually interesting images, but also helps to convey a story that might otherwise be lost with a typical graph, namely seeing how a text becomes positive or negative over its duration.\n\nShow code\n# trying to make something that works vaguely like: https://datatricks.co.uk/the-hidden-art-in-pi\n\n# use these: 33, 215\n\n# Unsorted affinity, so the book from start to finish\n\nsentiment_snake <- function(text_num){\n  text <- gutenberg_download(text_num) %>%\n  mutate(text = str_to_lower(text, locale = \"en\"))\n\nplot_title <- gutenberg_works(gutenberg_id == text_num) %>%\n  select(title) %>%\n  as.character()\n\ntext_afinn_unsort <- text %>%\n  unnest_tokens(output = word, input = text, token = \"words\") %>%\n  inner_join(afinn, by = \"word\") \n\ntext_unsort_simp <- text_afinn_unsort %>%\n  mutate(value = case_when(value == 1 ~ 1,\n                           value == 2 ~ 2,\n                           value == -1 ~ -1,\n                           value == -2 ~ -2,\n                           value %in% 3:5 ~ 3,\n                           value %in% -3:-5 ~ -3))\n\nbook_coords <- text_unsort_simp %>%\n  select(value) %>%\n    mutate(sign = sign(value),\n         x_coord = cumsum(sign),\n         y_coord = cumsum(value))\n\nbook_coords_7deg <- text_unsort_simp %>%\n  mutate(x_coord = cumsum(case_when(value == 1 ~ .78,\n                             value == 2 ~ .97,\n                             value == 3 ~ .43,\n                             value == -1 ~ -.78,\n                             value == -2 ~ -.97,\n                             value == -3 ~ -.43)),\n         y_coord = cumsum(case_when(value == 1 ~ .62,\n                             value == 2 ~ -.22,\n                             value == 3 ~ -.9,\n                             value == -1 ~ .62,\n                             value == -2 ~ -.22,\n                             value == -3 ~ -.9)))\n\ndf <- tibble(x = 0, y = 0)\n\nggplot(data = book_coords_7deg, mapping = aes(x = x_coord, y = y_coord)) +\n  geom_path(color = \"white\") +\n  geom_point(data = df, mapping = aes(x = x, y = y, color = \"red\")) +\n  ggtitle(plot_title) +\n  theme(plot.title = element_text(hjust = 0.5, \n                                  family = \"Minerva Modern\", \n                                  size = 10, \n                                  color = \"#36454f\"),\n        legend.position = \"none\",\n        axis.title.x = element_blank(),\n        axis.text.x = element_blank(),\n        axis.ticks.x = element_blank(), \n        axis.title.y = element_blank(),\n        axis.text.y = element_blank(),\n        axis.ticks.y = element_blank(),\n        panel.grid.major = element_blank(),\n        panel.grid.minor = element_blank(),\n        panel.background = element_rect(fill = \"#36454f\",\n                                colour = \"#36454f\",\n                                size = 0.5, linetype = \"solid\"))\n}\n\n\n\n\nShow code\nlibrary(patchwork)\n\nsentiment_snake(215) +\n  sentiment_snake(33) + \n  sentiment_snake(345) + \n  sentiment_snake(12) + \n  sentiment_snake(68) + \n  sentiment_snake(132) + \n  sentiment_snake(403) + \n  sentiment_snake(514) + \n  sentiment_snake(28) +\n  sentiment_snake(36) + \n  sentiment_snake(61) + \n  sentiment_snake(78) + \n  sentiment_snake(113) + \n  sentiment_snake(120) + \n  sentiment_snake(139) + \n  sentiment_snake(140) + \n  plot_layout(ncol = 4)\n\n\n\n\nLet’s look at just one example, Call of the Wild, in closer detail so you can better see exactly what is happening.\n\nShow code\nsentiment_snake(215)\n\n\n\n\nOur last art generation takes our data art to the next level. The rayshader package contains a save_3Dprint function that can turn 3D models into printable files. This function gives us the potential to turn our data art into a physical object. We generate art for the rayshader models using cellular automata. Cellular automata are a complex mathematical concept that even we cannot fully explain despite having worked with cellular automata to generate this art. Wikipedia defines a cellular automaton as “a discrete model of computation.” Probably a more helpful explanation can be found in youtube videos and tutorials. From our own research we’ve come to understand a cellular automaton as a collection of cells in a grid. The simplest version of a cellular automaton has 2 states, on and off which are indicated by black and white, respectively. A cell that is turned on has the power to decide the state of surrounding cells depending on a neighborhood which is specified in the creation of the automaton. When talking about cellular automata, neighborhoods are certain patterns of cells that are created by mathematical functions. Different neighborhoods give an automaton different visual behaviors. A blog post by Antonio Sánchez Chinchón lists all of the parameters required to produce a cellular automaton in R and also, provides the code that helped us create our art.\nWe start with the same first steps at the previous art generation experiments. After loading all of the necessary libraries, we perform sentiment analysis to help decide our automaton parameters. In this case, we used the nrc lexicon in the get_sentiments function instead of the afinn lexicon. While the afinn lexicon performs a numerical sentiment analysis, the nrc lexicon performs analysis by categorizing words into one of 8 sentiments. We can later use the top sentiment of a text to decide the neighborhood of our automaton. In the next steps we join the Wikipedia color table with the ngram data frame of the selected text. In this example, we look at Alice’s Adventure in Wonderland.\nWe finally start building our cellular automaton. We pull Antonio Chinchón’s C++ functions from his github to get started. Feeding this code into R is easy. Simply create a new C++ file in R and paste the code from github. Save the C++ file and return to your R project file. Use the sourceCpp() function to feed the C++ code into your R code. Voila! These functions are performing mathematical tasks to ensure the proper operation of our cellular automaton. Honestly, as R coders, the details of these functions purposes are a mystery to us. We only know as much as the code comments tell us. Moving on…\nNext, we pull code for two more functions from Antonio Chinchón’s github. The initial_grid function does exactly what the name suggests: create the initial grid for the automaton. Not much creativity is involved in the grid creation, so we leave this code as is from the github. The convolution_indexes function creates neighborhoods to give our automatons. While this code requires creativity, it also requires an understanding of complex mathematical concepts. After extensive research of cellular automata neighborhoods, we chose 8 neighborhood formulas from the original github function based on how well we thought the patterns represented each nrc sentiment (a very scientific and objective process). Now, we have written all of the necessary functions to create our own cellular automata.\nWe have to create a data frame before we can plot our cellular automaton. We provide the automaton with all of the parameters that Antonio Chinchón’s blog post specifies. To more thoroughly explain each parameter:\n#add bullet points here! range tells the automaton how far each neighborhood extends from a cell. threshold tells the automaton how many different states a cell can reach before the automaton moves on to another cell. states tells the automaton how many different varieties of cells or how many different colors can exist in the plot. We have already discussed the role of the neighborhood in creating the automaton. iter, and abbreviation of iteration, tell the automaton how many times the it should run through the model. Finally, the width and height decide the dimensions of your final plot.\nThese parameters cannot be given a random set of numbers and be expected to generate an interesting visual pattern. Most number combinations produce what can only be described as color static. Jason of softologyblog provides some number combinations to begin with when trying to produce interesting visuals with cellular automata. We started with combinations that allowed a large number of cell states, because we wanted to include at least the top ten colors mentioned in a text. Plus, more color makes our art pieces more interesting.\nObject X creates the automaton’s grid. Object L runs the automaton using a for loop. The larger the specified width and height of your plot, the longer the for loop will take to run. Antonio Chinchón’s github example of an automaton had a width and height of 1500 and took upwards of five minutes to run the for loop. For the sake of experimentation, we kept our dimensions relatively small. Larger dimensions would create higher quality, less pixelated automaton images by including more cells and much smaller cells.\nThe melt function turns the output of the for loop into a tidy data frame with every row as an observation.\nUse the data frame to plot the automaton with the geom_raster function. Within the geom_raster function, interpolate = TRUE smoothens out the plot image, and the theme_nothing function creates an output with the appearance of an image instead of a graph. Once we produced an output that we were happy with, we turned the plot into a ggobject. This object gets put into the rayshader plot_gg function. The rayshader models can only be viewed on a local installation of R. An R studio server will not be able to produce the rgl device window which is necessary to view 3D models. If using a Mac, make sure to download and open XQuartz before running the rayshader code. Macs will not be able to open an rgl device window without XQuartz. Once you have adjusted the rayshader code to produce a model of your liking, use the save_3Dprint function to produce a file that you can take to your local 3D printer, wherever that may be.\nTime permitting, we would have loved to take this project a step further in terms of reproducibility.We believe that a function that automates the creation of a text data cellular automaton is a time-consuming but worthwhile project for the future. In our vision, the function requires a single input: a gutenbergr text number. The function would perform the same nrc text analysis we already wrote to identify the given text’s strongest sentiment. The function would then choose the neighborhood that we would have liked to match to the identified popular sentiment. Finally, the function would choose the cellular automaton’s 10 colors using the color and text code we’ve already written. There would be two outputs: the data frame and the rayshader plot. In this imagined function, the number of range, states, and threshold would always stay the same. The iterations, width, and height would be optional, adjustable inputs for the function. Ultimately, we would be able to produce unique art pieces with one line of code, and we would be able to make a physical copy with just one more line. All we would need is two lines of code to become an artist! Imagine that…\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-10T11:29:11-07:00",
    "input_file": {}
  }
]
